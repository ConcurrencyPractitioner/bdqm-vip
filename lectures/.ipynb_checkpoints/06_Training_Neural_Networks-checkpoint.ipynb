{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Force Fields\n",
    "\n",
    "A lot of what we do in our group involves the use of what are called \"neural network forcefields.\" These are basically energy and force calculators that are just neural networks that have been trained against some data (be it quantum mechanical/classical/semi-empircal/ect...) It is trained to recreate the potential energy surface. \n",
    "\n",
    "For training neural network force fields, we will be using AMPtorch which is a pytorch implementation of Atomistic Machine-learning Package (AMP). AMP is an open-source code which has been developed by our collaborators at Brown University. More details about the package can be found [here](https://amp.readthedocs.io/en/latest/index.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory\n",
    "\n",
    "According to the Born-Oppenheimer approximation, the potential energy at ground state for a system is dictated by nuclear coordinates when there are no external fields and constant charges. The potential energy can be considered as a function of the nuclear coordinates of a system. Since it is difficult to get an exact analytical solution to many-body schrodinger equation, we can make use of a electronic structure calculator to approximate the ground-state energy by using a regression model:\n",
    "\n",
    "$E = E(R)$ where R is position of atoms in the system\n",
    "\n",
    "Here, the total potential energy of the system can be broken down to take into account the atomic energy contributions:\n",
    "\n",
    "$E_{atom} = E_{atom}(R)$\n",
    "\n",
    "$E(R) = \\sum\\limits _{atoms=1} ^{N} E_{atom}(R)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Descriptors\n",
    "\n",
    "In AMP, we use appropriate symmetry function **G** of atomic coordinates as an input into regression operator. This symmetry function **G** approximates the functional dependence of local energetics. Hence, our equation modifies into:\n",
    "\n",
    "$E_{atom} = E_{atom}(G(R))$\n",
    "\n",
    "![AMP scheme](amp.png)\n",
    "\n",
    "The symmetry functions will create the \"feature vectors\" which can be fed into the machine-learning regression model. There are different descriptors implemented within AMP code:\n",
    "\n",
    "1. Gaussian\n",
    "2. Zernike\n",
    "3. Bispectrum\n",
    "\n",
    "For the purpose of this lecture and research work, we will be focussing on Gaussian descriptors. These descriptors were suggested by Behler and more details can be found [here](https://aip.scitation.org/doi/10.1063/1.3553717). \n",
    "\n",
    "There are two kinds of descriptors: radial ($G_2$) and angular ($G_4$). The radial descriptors capture the interaction of atom *i* with atom *j* as a sum of Gaussians with width $\\eta$ and center (offset) $R_s$. The angular descriptors account for three-atom interactions.  \n",
    "\n",
    "$G_{2} = \\Sigma_{i \\neq j} exp(-\\eta (\\frac{R_{ij}}{R_{c}})^2)f_C(R_{ij})$\n",
    "\n",
    "For $G_2$ you can pick the value of $\\eta$, the fall off speed and $R_c$, the cutoff distance\n",
    "\n",
    "\n",
    "$G_{4} = 2^{1-\\zeta} \\Sigma_i\\Sigma_j\\Sigma_{k i \\neq j \\neq k} (1+\\gamma cos(\\Theta_{jik}))^{\\zeta}exp(-\\eta \\frac{R_{ij}^2 +R_{ik}^2+R_{jk}^2}{R_{c}^2}) f_C(R_{ij})f_c(R_{ij})f_c(R_{ik})f_c(R_{kj})$\n",
    "\n",
    "For $G_4$ you can pick $\\eta$, $\\zeta$, $\\gamma$, and $R_c$\n",
    "\n",
    "The cutoff function is:\n",
    "\n",
    "$f_c(R_{ij}) = \\frac{1}{2}(1 + cos(\\pi \\frac{Rij}{Rc}))$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression model\n",
    "\n",
    "The regression model will take input as the feature vectors and output as the properties (that we expect to predict using the model) and approximate the function $y = f(x)$ by using sample training data points $(x_i, y_i)$. AMP uses a neural network model to approximate this function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation\n",
    "\n",
    "For the purpose of this course, we will install the refactored version of AMPtorch. This has been developed in collaboration with our collaborators at CMU and Brown. The installation of AMPtorch is an easy process and can be accessed [here](https://github.com/ulissigroup/amptorch/tree/refactor) and is done using a conda environment. You can create the conda environment for either CPU or GPU machines once you have cloned the github repository. \n",
    "\n",
    "For cloning the refactor branch of AMPtorch code on command line, run:\n",
    "\n",
    "*git clone --single-branch --branch refactor https://github.com/ulissigroup/amptorch.git*\n",
    "\n",
    "Once you have the repo on your local workstation or PACE, you can create the environment, activate it and install the package. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will look into how to get this running. An example script can be found [here](https://github.com/ulissigroup/amptorch/blob/refactor/examples/example.py). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
